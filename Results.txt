Improving DQN methods: https://cugtyt.github.io/blog/rl-notes/201807201658.html

######################################### Vanilla DQN #########################################

*) Model1:
	Warmup: 500
	Num_episodes: 5000
	EMC: 15%
	Training Frequency: 100
	Maximum Memory size = 5000
	
	- Results:
		training_history = ['Episode 1000 => Best success rate = 0.42', 'Episode 2000 => Best success rate = 0.69', 'Episode 3000 => Best success rate = 0.77', 'Episode 4000 => Best success rate = 0.78',
					 'Episode 5000 => Best success rate = 0.85']

		Test / 100 episode = 83% - 77% - 78%


*) Model2:
	Warmup: 500
	Num_episodes: 10000
	EMC: 15%
	Training Frequency: 100
	Maximum Memory size = 5000

	- Results:
		training_history = ['Episode 1000 => Best success rate = 0.86', 'Episode 2000 => Best success rate = 0.93', 'Episode 3000 => Best success rate = 0.93', 'Episode 4000 => Best success rate = 0.93',
				    'Episode 5000 => Best success rate = 0.99', 'Episode 6000 => Best success rate = 0.99', 'Episode 7000 => Best success rate = 0.99', 'Episode 8000 => Best success rate = 0.99',
				    'Episode 9000 => Best success rate = 0.99', 'Episode 10000 => Best success rate = 0.99']

		Test / 100 episodes = 87% - 92% - 93%





######################################### DDQN #########################################
Double DQN: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action.
reference: https://datascience.stackexchange.com/questions/32246/q-learning-target-network-vs-double-dqn

*) Model3
	Warmup = 500
	Num_episodes = 10000
	EMC = 15%
	Training Frequency = 100
	Maximum memory size = 5000

	- Results:
		training_history = ['Episode 1000 => Best success rate = 0.57', 'Episode 2000 => Best success rate = 0.67', 'Episode 3000 => Best success rate = 0.81', 'Episode 4000 => Best success rate = 0.82', 
					'Episode 5000 => Best success rate = 0.84', 'Episode 6000 => Best success rate = 0.91', 'Episode 7000 => Best success rate = 0.92', 'Episode 8000 => Best success rate = 0.94',
					 'Episode 9000 => Best success rate = 0.94', 'Episode 10000 => Best success rate = 0.95']
		
		Test / 100 episodes = 90% - 91%



######################################### Vanilla DQN without Fixed Q Targets #########################################

*) Model4
	Warmup = 500
	Num_episodes = 10000
	EMC = 15%
	Training Frequency = 100
	Maximum memory size = 5000

	- Results:
		training_history = ['Episode 1000 => Best success rate = 0.56', 'Episode 2000 => Best success rate = 0.82', 'Episode 3000 => Best success rate = 0.86', 'Episode 4000 => Best success rate = 0.9', 'Episode 5000 => Best success rate = 0.97', 'Episode 6000 => Best success rate = 0.99', 'Episode 7000 => Best success rate = 0.99', 'Episode 8000 => Best success rate = 0.99', 'Episode 9000 => Best success rate = 0.99', 'Episode 10000 => Best success rate = 0.99']
		
		Test / 100 episodes = 87% - 93% - 94%
























